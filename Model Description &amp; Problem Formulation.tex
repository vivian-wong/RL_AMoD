%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
\usepackage{graphicx} 
\usepackage{algpseudocode,algorithm}
\usepackage{bm}
\usepackage{color}
\usepackage{subfig}
\usepackage{url}
\usepackage{floatrow}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf Reinforcement Learning in AMoD\\
Model Description \& Problem Formulation
}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\author{Vivian Wong}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%-BEGIN-%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Model and Problem Formulation}
In this section, we first outline the background information that helps define variables in the model. We then describe the Markov Decision Process (MDP) used to model vehicle rebalancing as a sequential decision-making process. Lastly, we present the problem formulation for the vehicle rebalancing problem in two stages: learning and rebalancing. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Background Information}
Consider an environment discretized into a set of $\mathcal{N}$ stations (also can be viewed as discretized regions, as in [1]). Time is presented in discrete intervals of unit time $1$. The furthest time that will be considered is denoted as $\mathcal{T}$.

Furthermore, let $M$ denote the total number of vehicles. $v_i$ denotes the number of idle vehicles, or vehicles waiting at station $i\in N$ and not serving any customer at time $t$. $v_{it'}$ denotes the number of vehicles that will arrive at station $i$, and will become an idle vehicle at $t'$. Note that we assume that the vehicles will arrive at time $t'$ deterministically, or that no uncertainty is taken into account for travel times between two stations. 

In an AMoD system, at every station there are customers demanding vehicles to service their transportation requests. We denote $\lambda_{ijt}$ as the number of customers waiting to get a vehicle at station $i$ at time $t$, wishing to travel to station $j$. $\lambda'_{ijt}$ represents the demand forecasted with a LSTM neural network, as described in [1].
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MDP Model Description}
At every time step $t \in \{1,...,T-1,T\}$, a MDP, an \textit{agent} observes the environment and store the values of interest in the observation as its \textit{current state, $s_t$}. The agent then conducts an \textit{action $a_t$} following a certain policy. At the end of this time step (or the beginning of time step $t+1$), a \textit{new state $s_{t+1}$} and a \textit{reward $r_t$} are observed. The definitions of MDP components are:\\

\noindent \textit{Agent}: Each vehicle is treated as an agent. Although this definition makes the problem a multiagent problem, it reduces the dimensions of actions, so that there is a smaller action space to explore. \\

\noindent \textit{State, $s_t$}: The state at time step $t$ is defined as a four-dimensional tuple, where each element of the tuple stores an array. Each element of the tuple stores the following in order: $v_i$ , $v_{it'}$, $\lambda_{ijt}$ and $\lambda'_{ijt} \forall i,j\in\mathcal{N}$. The dimensions of the four elements are $|\mathcal{N}|,|\mathcal{N}|T,|\mathcal{N}|^2,|\mathcal{N}|^2$ respectively.

\noindent \textit{Action, $a_t$}: An agent is only made to choose an action when it is idle. Consider an idle agent currently at station $i$. Its action is defined as $j$, where $j\in \mathcal{N}$. If $i = j$, then the agent remains idle at its current station. If $i \neq j$, the agent departs to go to station $j$. The dimension of the action space is therefore $\mathcal{N}$. \\

\noindent \textit{Reward, $r_t$}: The reward is equal to the average customer wait time at time $t$. \\ 

Lastly, from classical MDP definitions, we define the state-action value, $Q(s,a)$ as the expected utility if following an action $a$ from state $t$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem Formulation}
\subsubsection{Learning Stage}
The goal in the learning stage is to learn the state-action values, $Q(s_t,a_t)$ at each explored state.

To update and estimate state-action values, we will use the Q-learning algorithm:
\begin{algorithm}[H]
\caption{Q-Learning}
\begin{algorithmic}
\For{each ($s_t,a_t,r_t,s_{t+1}$)}
    \State $Q_{target} = r_T+\gamma \max_{a_{t+1}} Q(s_{t+1},a_{t+1})$
    \State $Q_{prediction} = Q(s_t,a_t) \leftarrow (1-\alpha)Q_{predict}+\alpha Q_{target}$
\EndFor
\end{algorithmic}
\end{algorithm}


The algorithm approaches convergence as the following objective is met:
\begin{equation}
        \min (Q_{target} - Q_{prediction})^2
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Rebalancing Stage}
Actions that give highest state-action values for each agent may be only optimal locally but not globally. At this stage, we aim to use the state-action values learned in the learning stage to find actions that minimize customer wait time by rebalancing empty vehicles. 

To do this, we find the global optimum by assigning actions sequentially for each vehicle. We create smaller simulated states, $\hat{s_t}$ that are updated every time a vehicle has been assigned an action. The algorithm follows: 
\begin{algorithm}[H]
\caption{Assigning Actions using Learned Values}
\begin{algorithmic}
\For{each time step $t \in {0,1,...,T}$}
    \State $\hat{s_t}=s_t$
    \For{each vehicle $m$ in $v_i_t$ vehicles}
        \If {vehicle is not idle}
            \State \textbf{Continue}
        \EndIf
        \State $a_t^m \leftarrow \argmax_a(Q(\hat{s_t},a))$
        \State $\hat{s_t}$ \leftarrow \textbf{simulate}$(a^m_t,s_t)$
    \EndFor
    \State\textbf{Assign} $\{a^1_t,a^2_t,...,a^M_t\}$
\EndFor
\end{algorithmic}
\end{algorithm}

This algorithm finds actions that aim for the following objective: 
\begin{equation}
        \min -\sum_m^M Q^m((v_i,v_it',\lambda_i,\lambda'_i),j)\forall i,j\in \mathcal{N}
\end{equation}
By creating smaller simulated states and thereby making decisions sequentially, we can take into account of the actions of other agents while optimizing actions for one agent.  

%%%%%%%%%%%%%%%%%%%%
\begin{thebibliography}{99}

\bibitem{c1} R. Iglesias, F. Rossi, K. Wang, D. Hallac, J. Leskovec, and M. Pavone, “Data-Driven Model Predictive Control of Autonomous Mobility-on-Demand Systems,” \textit{Proc. IEEE Conf. on Robotics and Automation}, 2018.


\end{thebibliography}
\end{document}
