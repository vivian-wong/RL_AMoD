\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{algpseudocode}

\title{Model Description & Problem Formulation}
\author{vwwong3 }
\date{November 2018}

\begin{document}

\maketitle

\section{MDP Model Description}
The AMoD rebalancing problem can be modeled as a MDP with the following components: \\

\noindent \textit{Time step, $t$}: In reality, it is more cost-effective to check on the status of all vehicles and do rebalancing regularly after a certain time period (e.g. 5 minutes). This time period can therefore be defined as a unit time 1, and each time the system checks on the status of the environment would be a time step $t \in \{0,1,...,T-1,T\}$, where $T$ is the furthest time step that will be observed. At the start of each time step $t$, an agent starts out in a current state $s_t$, carries out an action $a_t$ following a certain policy. At the end of this time step (or the beginning of time step $t+1$), a new state $s_{t+1}$ and a reward $r_t$ is observed.\\

\noindent \textit{Agent}: Each vehicle is treated as an agent. Although this definition makes the problem a multiagent problem, it reduces the dimensions of actions, so that there is a smaller action space to explore. \\

\noindent \textit{State, $s_t$}: The state at time step $t$ is defined as a four-dimensional tuple, $(v_i,v_{it'},\lambda_i,\lambda'_i)$.

The first two terms give information about all vehicles in the system. Consider a total of $N$ stations in the environment. $v_i$ represents the number of idle vehicles, or vehicles waiting at station $i\in N$ and not serving any customer at time $t$. $v_i$ is an array of dimension $i$. $v_{it'}$ is the number of vehicles that will deterministically arrive at station $i$, and will become an idle vehicle at $t'$, where $t'\in \{(t,T)\}$ ($T$, once again, is the maximum time that will be observed). This term is an array with a maximum dimension (worst case) of $min(M,N(T-t))$, where $M$ is the total number of vehicles.

The last two terms give information about the customer demand. $\lambda_i$ represents the number of customers waiting to get a vehicle at station $i$. $\lambda'_i$ represents the demand forecasted with a LSTM neural network. The dimensions of these two terms are $N$.

With this state definition, each agent can observe the entire macroscopic environment.\\

\noindent \textit{Action, $a_t$}: An agent is only made to choose an action when it is idle. Consider an idle agent currently at station $i$. Its action is defined as $j$, where $j\in N$. If $i = j$, then the agent remains idle at its current station. If $i != j$, the agent departs to go to station $j$. The dimension of the action space is therefore $N$. \\

\noindent \textit{Reward, $r_t$}: Is the average customer wait time. \\ 

\noindent Lastly, from classical MDP definitions, we define:

State-action values (expected utility if follow an action from state), $Q(s,a)$: 

Values (expected utility), $ V(s)$  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Formulation }
\subsection{Learning Stage}
The goal in the learning stage is to learn the state-action values, $Q(s_t,a_t)$ at each state.

To estimate the optimal values, we will use the Q-learning algorithm:
\begin{algorithmic}
\For{each ($s_t,a_t,r_t,s_{t+1}$)}
    \State $Q_{target} = r_T+\gamma \max_{a_{t+1}} Q(s_{t+1},a_{t+1})$
    \State $Q_{prediction} = Q(s_t,a_t) \leftarrow (1-\alpha)Q_{predict}+\alpha Q_{target}$
\EndFor\\
\end{algorithmic}

The algorithm approaches convergence as the following objective is met:
\begin{equation}
        \min (Q_{target} - Q_{prediction})^2
\end{equation}

\subsection{Rebalancing Stage}

At this stage, given the state-action values $Q$ from the learning stage and the current state, we want to find actions that maximize the total state-action values, in order to achieve global optimal. \\
% optimal action for each vehicle may not be optimal action for global, because of constraints? 

To do this, we formulate the following optimization problem. Consider a system of $M$ vehicles in total, and each vehicle could be indexed as $m$.
\begin{equation}
        \min -\sum_m^M Q_m((v_i,v_it',\lambda_i,\lambda'_i),j)
\end{equation}
where $Q_m$ is the Q value function obtained in the learning stage, for vehicle/agent $m$.\\

The constraints are that inflow at all nodes must equal outflow: 
\begin{itemize}
    \item if there are $v_{it'}$ vehicles planning to arrive at station $i$ at time $t'$, then the number of idling vehicles at station $i$ at time $t'$ must be equal to $v_{it'}$
    \item the number of vehicles heading towards station $i$ at time step $t$, must be equal to that plus the number of  idling vehicles at station $i$ at time step $t+1$
\end{itemize}
Formally, the constraints above can be written as: 
\begin{equation}
    \begin{array}{l}
         v_{it'}(t) = v_i(t'), \forall i\in N \\
         \sum_{t'=t+1}^T v_{it'} + \sum_{m=1}^M 1(a_t_m == j) = \sum_{t'=t+2}^T v_{it'} + v_j(t+1)
    \end{array}
\end{equation}
\end{document}
